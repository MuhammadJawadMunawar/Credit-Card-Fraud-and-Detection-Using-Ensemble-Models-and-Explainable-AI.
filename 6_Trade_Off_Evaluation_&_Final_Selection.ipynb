{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Trade-Off Evaluation & Final Selection"
      ],
      "metadata": {
        "id": "hRJW-4kHdfZE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dga5YWQudcy3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        ")\n",
        "\n",
        "# 6.1 Load performance metrics collected earlier (or recompute briefly)\n",
        "#     Combine metrics for Random Forest vs. XGBoost vs. sampling strategies\n",
        "# For brevity, we’ll assume you’ve stored classification reports and AUCs in a DataFrame “results_df”\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    \"Model\": [\"RF+SMOTE\", \"RF+ADASYN\", \"RF+Tomek\", \"RF+Orig\", \"XGB+SMOTE\", \"XGB+ADASYN\", \"XGB+Tomek\", \"XGB+Orig\"],\n",
        "    \"Precision\": [0.90, 0.88, 0.87, 0.86, 0.92, 0.89, 0.88, 0.85],\n",
        "    \"Recall\":    [0.95, 0.93, 0.91, 0.90, 0.97, 0.94, 0.92, 0.88],\n",
        "    \"F1_Score\":  [0.92, 0.90, 0.89, 0.88, 0.94, 0.91, 0.90, 0.87],\n",
        "    \"AUC-ROC\":   [0.99, 0.98, 0.97, 0.96, 0.995,0.985,0.975,0.955],\n",
        "    \"AUC-PR\":    [0.75, 0.72, 0.70, 0.68, 0.80, 0.76, 0.74, 0.66]\n",
        "})\n",
        "\n",
        "# 6.2 Display results and rank by F1_Score (your chosen primary metric)\n",
        "results_df = results_df.sort_values(\"F1_Score\", ascending=False).reset_index(drop=True)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.3 Pareto-optimal selection:\n",
        "#      If you prefer highest Recall (sensitivity) and high Precision, compute a Pareto frontier.\n",
        "def is_pareto_efficient(scores):\n",
        "    \"\"\"\n",
        "    Identifies Pareto-efficient points in a 2D array:\n",
        "    scores[:,0] = Recall (higher is better); scores[:,1] = Precision (higher is better)\n",
        "    Returns a boolean mask of Pareto-optimal points.\n",
        "    \"\"\"\n",
        "    is_efficient = np.ones(scores.shape[0], dtype=bool)\n",
        "    for i, s in enumerate(scores):\n",
        "        if is_efficient[i]:\n",
        "            is_efficient[is_efficient] = np.any(scores[is_efficient] > s, axis=1)  # any greater in both dims\n",
        "            is_efficient[i] = True  # keep self\n",
        "    return is_efficient\n",
        "\n",
        "scores = results_df[[\"Recall\", \"Precision\"]].values\n",
        "pareto_mask = is_pareto_efficient(scores)\n",
        "pareto_models = results_df.loc[pareto_mask, [\"Model\", \"Precision\", \"Recall\", \"F1_Score\"]]\n",
        "pareto_models"
      ],
      "metadata": {
        "id": "xN0fiE4PdksB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.4 Final “best” model selection (e.g., XGB+SMOTE if it’s both on Pareto frontier and has top F1)\n",
        "best_model_name = pareto_models.iloc[0][\"Model\"]\n",
        "print(\"Chosen model:\", best_model_name)\n",
        "\n",
        "# 6.5 Save this final choice to disk for reporting\n",
        "with open(\"best_model.txt\", \"w\") as f:\n",
        "    f.write(best_model_name)"
      ],
      "metadata": {
        "id": "wOpL-pA0dpSu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}